# üåç Safe Heaven
**Empowering Digital Spaces with AI-Driven Moderation**

Safe Heaven is an **AI-powered online harm detection system** designed to create a safer digital environment by detecting and moderating harmful content. It leverages **Natural Language Processing (NLP) and Machine Learning (ML)** techniques to identify **hate speech, cyberbullying, and harmful text** in real-time.

## üöÄ Features
- **Hate Speech Detection**: Identifies and flags offensive or harmful language.
- **Content Moderation**: Filters and categorizes user-generated content based on severity.
- **Community Forum**: A safe space for discussions with AI-powered moderation.
- **Reporting Tool**: Enables users to report harmful content for further analysis.
- **Resource Library**: Educates users on hate speech, cyberbullying, and digital safety.

## üõ†Ô∏è Technologies Used
- **Frontend**: HTML, CSS, JavaScript
- **Backend**: Node.js, Express.js
- **Database**: MongoDB (for storing posts and reports)
- **Machine Learning Models**: TensorFlow, Scikit-learn, Transformers (BERT)

## ü§ñ AI Models
Safe Haven employs **two powerful approaches** for detecting and moderating content:

### 1Ô∏è‚É£ BERT-Based Hate Speech Detection
- Utilizes **Bidirectional Encoder Representations from Transformers (BERT)**.
- Pre-trained on large datasets and fine-tuned for **hate speech classification**.
- Provides highly accurate detection by understanding contextual meaning.
- Categorizes content as **neutral, offensive, or highly toxic**.

### 2Ô∏è‚É£ Sentiment Analysis-Based Moderation
- Uses **Natural Language Processing (NLP)** techniques to assess sentiment.
- Determines if content is **positive, neutral, or negative**.
- Flags content with a strong negative sentiment for further moderation.
- Ideal for detecting **cyberbullying and harassment**.

## üõ°Ô∏è Why Safe Heaven?
Hate speech and harmful online interactions are growing concerns in the digital world. Safe Heaven aims to **empower users and communities** with an AI-driven solution to:
- **Detect harmful content in real time**.
- **Encourage healthy discussions**.
- **Promote digital well-being and inclusivity**.

---
üöÄ *Building a safer online space, one step at a time!*

# Safe-Heaven
## Detecting Hate Speech with Sentiment Analysis and ML
Safe Haven is an AI-powered platform designed to combat online harms by detecting hate speech effectively and efficiently. By leveraging sentiment analysis and machine learning (ML), this project aspires to create a safer digital environment by identifying and moderating harmful content. The project incorporates two models:

A Python-based model, built from scratch using Google Colab, designed for high accuracy in detecting online harms.

A JavaScript inbuilt library with adjustable sentiment analysis and moderation levels, allowing customizable sensitivity.

The Python model has shown superior performance in identifying hate speech, demonstrating the potential of advanced ML methods in safeguarding online communities.

## Table of Contents
1. [About the Project](#about-the-project)
2. [Features](#features)
3. [Technologies Used](#technologies-used)
4. [Setup and Installation](#setup-and-installation)
5. [Usage](#usage)
## About the Project
Safe Haven was developed as part of the GDG Solutions Hackathon 2025, addressing the challenge "Scaling Trust: AI-Powered Detection of Online Harms." This innovative project aims to mitigate online harms by identifying and moderating hate speech using cutting-edge sentiment analysis and machine learning techniques.

The project consists of two models:

Python Model: Built from scratch, it employs advanced ML algorithms for reliable detection of hate speech.

JavaScript Model: Utilizes a sentiment analysis library with adjustable moderation levels, providing flexibility to adapt to varying requirements.

## Features
*Dual-Model System: Combines the robustness of the Python model with the flexibility of the JavaScript library.

*Effective Hate Speech Detection: The Python model outperforms in identifying harmful content.

*Adjustable Moderation Levels: The JavaScript library allows users to customize sensitivity according to their needs.

*Real-time Processing: Both models efficiently analyze input to provide immediate results.

*Scalability: Designed to handle high-volume online data streams.

## Technologies Used
**Python**: Core programming language for the first model (developed in Google Colab).

**JavaScript**: Used in the second model for sentiment analysis.

**Node.js**: Backend runtime environment for the JavaScript model.

**React.js**: Frontend library to build an intuitive user interface.

**MongoDB and Mongoose**: Database and ORM (Object Relational Mapper) for seamless data management.

**Express.js**: Framework to streamline server-side development.

**Machine Learning Libraries**: Supporting the Python model.

## Setup and Installation
## Python Model
1. Open Google Colab and upload the Python script.

2. Install the necessary libraries by running:
   pip install -r requirements.txt
3. Run the script to train and deploy the model.

## JavaScript Model
1. Clone the repository from GitHub:
   git clone https://github.com/Su-creator-spec/safe-heaven/tree/main
2. Navigate to the project directory:
   cd safe-heaven
3. Install the required dependencies:
   npm install


## Usage
## Python Model
*Execute the script in Google Colab to analyze data and detect hate speech. The results will be generated in real-time.
## JavaScript Model
Run the application by starting the server:
npm start
*Adjust the moderation levels for tailored sensitivity to harmful content.


